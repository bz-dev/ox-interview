{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Introduction\n",
    "\n",
    "The following datasets are used in this demonstration:\n",
    "- 2011 Census data - population and household estimates, available [here](https://www.ons.gov.uk/census/2011census/2011censusdata/2011censusdatacatalogue/populationandhouseholdestimates).\n",
    "- ONS - House price to residence-based earnings ratio, available [here](https://www.ons.gov.uk/peoplepopulationandcommunity/housing/datasets/ratioofhousepricetoresidencebasedearningslowerquartileandmedian)\n",
    "- Kaggle - COVID19 related tweets, available [here](https://www.kaggle.com/gpreda/covid19-tweets)\n",
    "\n",
    "These datasets are also available altogether [here](https://github.com/bz-dev/ox-interview/tree/main/data) in the GitHub repository for this demonstration.\n",
    "\n",
    "### Table of contents\n",
    "\n",
    "1. [Population and household estimates (**univariate**)](#section1)\n",
    "\n",
    "    1.1 [Gender ratio](#section1_1)\n",
    "\n",
    "    <small>Basic pie chart</small>\n",
    "\n",
    "    1.2 [Population and gender ratio by region](#section1_2)\n",
    "\n",
    "    <small>How to use subplots to group charts together, and how to use annotations to add more details to the plot.</small>\n",
    "\n",
    "    1.3 [Population by outward postcode using choropleth map](#section1_3)\n",
    "\n",
    "    <small>How to plot location related data on a map using geojson.</small>\n",
    "\n",
    "    1.4 [Population by postcode parent area using choropleth map](#section1_4)\n",
    "\n",
    "    <small>How to merge granulated geo areas and plot on the map.</small>\n",
    "\n",
    "2. [House prices and earnings (**multivariate**)](#section2)\n",
    "\n",
    "    2.1 [Correlation between median house price and median household earning](#section2_1)\n",
    "\n",
    "    <small>How to use a combination of scatter plot, trendline, box plot and rug plot to show distributions and correlations between variables.</small>\n",
    "\n",
    "    2.2 [Median and lower quartile house prices from 2002 to 2020 (animated)](#section2_2)\n",
    "\n",
    "    <small>How to use animated chart to show trends over changes of one variable.</small>\n",
    "\n",
    "3. [Text analysis and visualisation](#section3)\n",
    "\n",
    "    3.1 [Text tokenization and word cloud](#section3_1)\n",
    "\n",
    "    <small>How to perform a simple text tokenization and use word cloud to show word frequencies.</small>\n",
    "\n",
    "    3.2 [Basic sentiment analysis with VADER](#section3_2)\n",
    "\n",
    "    <small>How to perform a simple sentiment analysis and use grouped line charts.</small>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<hr>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Preparation\n",
    "Please run this code block before running any other blocks in this notebook."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Install packages using jupyter notebook's built-in %pip function\n",
    "%pip install pandas plotly geojson shapely openpyxl scipy nltk\n",
    "\n",
    "# Import required packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from pathlib import Path\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import re\n",
    "import shapely.geometry\n",
    "from shapely.ops import unary_union\n",
    "import geojson\n",
    "import itertools\n",
    "import wordcloud\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# Set default pandas plotting backend to Plotly\n",
    "pd.options.plotting.backend = \"plotly\"\n",
    "\n",
    "# Pass paths to resources to variables\n",
    "file_population = Path(\"../data/ons/population_household.csv\")\n",
    "dir_geojson = Path(\"../data/geojson\")\n",
    "file_house_price = Path(\"../data/ons/house_price_earning.xlsx\")\n",
    "file_tweet = Path(\"../data/tweet/covid19_tweets.csv\")\n",
    "\n",
    "print(\"✅ Notebook preparation completed.\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id='section1'></a>\n",
    "### 1. Population and household estimates"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Declare section parent variable\n",
    "var1 = dict()\n",
    "\n",
    "# Load data\n",
    "var1[\"df\"] = pd.read_csv(file_population)\n",
    "\n",
    "# Split postcode into outward and inward, e.g. LE3 9QP => LE3 and 9QP\n",
    "var1[\"df\"][\"postcode_out\"] = var1[\"df\"][\"Postcode\"].apply(lambda x: x[0:4].strip())\n",
    "var1[\"df\"][\"postcode_in\"] = var1[\"df\"][\"Postcode\"].apply(lambda x: x[4:].strip())\n",
    "\n",
    "# Keep only non-numeric part in outward as the parent region\n",
    "var1[\"df\"][\"postcode_region\"] = var1[\"df\"][\"postcode_out\"].apply(lambda x: re.split(\"\\d+\", x)[0])\n",
    "\n",
    "# Remove column [Postcode] to save memory\n",
    "var1[\"df\"] = var1[\"df\"].drop([\"Postcode\"], axis=1)\n",
    "\n",
    "print(\"✅ Section 1 preparation completed.\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id='section1_1'></a>\n",
    "#### 1.1 Gender ratio"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Declare section parent variable\n",
    "sec1_1 = dict()\n",
    "\n",
    "# Count total number of males and females\n",
    "sec1_1[\"df\"] = pd.DataFrame(\n",
    "    dict(Gender=[\"Male\", \"Female\"], Count=[var1[\"df\"][\"Males\"].sum(), var1[\"df\"][\"Females\"].sum()]))\n",
    "\n",
    "# Generate pie chart\n",
    "sec1_1[\"fig\"] = px.pie(sec1_1[\"df\"], values=\"Count\", names=\"Gender\", title=\"National gender ratio (England and Wales)\")\n",
    "sec1_1[\"fig\"].show()\n",
    "\n",
    "# Remove variable to save memory\n",
    "del sec1_1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id='section1_2'></a>\n",
    "#### 1.2 Population and gender ratio by region"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Declare section parent variable\n",
    "sec1_2 = dict()\n",
    "\n",
    "# Sum up all numbers within same postcode region, descending order by column [Total]\n",
    "sec1_2[\"df\"] = var1[\"df\"].groupby([\"postcode_region\"]).sum().sort_values(by=['Total'], ascending=False).reset_index()\n",
    "\n",
    "# Generate plots side by side\n",
    "sec1_2[\"fig\"] = make_subplots(rows=1, cols=2, specs=[[{}, {}]], shared_xaxes=True,\n",
    "                              shared_yaxes=False, vertical_spacing=0.001)\n",
    "\n",
    "# Add plot 1: bar chart for top 10 population regions\n",
    "sec1_2[\"fig\"].append_trace(go.Bar(\n",
    "    x=sec1_2[\"df\"].head(10)[\"Total\"],\n",
    "    y=sec1_2[\"df\"].head(10)[\"postcode_region\"],\n",
    "    marker=dict(color='rgba(50, 171, 96, 0.6)', line=dict(\n",
    "        color='rgba(50, 171, 96, 1.0)',\n",
    "        width=1)),\n",
    "    name='Population',\n",
    "    orientation='h',\n",
    "), 1, 1)\n",
    "\n",
    "# Add plot 2: scatter chart for gender ratio in top 10 population regions\n",
    "sec1_2[\"fig\"].append_trace(\n",
    "    go.Scatter(\n",
    "        x=sec1_2[\"df\"].head(10)[\"Males\"] / sec1_2[\"df\"].head(10)[\"Females\"],\n",
    "        y=sec1_2[\"df\"].head(10)[\"postcode_region\"],\n",
    "        mode='lines+markers', line_color='rgb(128, 0, 128)', name='M/F ratio',\n",
    "    ), 1, 2)\n",
    "\n",
    "# Update layout changes on plot title, axes, color and legends\n",
    "sec1_2[\"fig\"].update_layout(\n",
    "    title='Male/Female ratio in top 10 population regions',\n",
    "    yaxis=dict(showgrid=False, showline=False, showticklabels=True, domain=[0, 0.85]),\n",
    "    yaxis2=dict(showgrid=False, showline=True, showticklabels=False,\n",
    "                linecolor='rgba(102, 102, 102, 0.8)', linewidth=2, domain=[0, 0.85]),\n",
    "    xaxis=dict(zeroline=False, showline=False, showticklabels=True,\n",
    "               showgrid=True, domain=[0, 0.42]),\n",
    "    xaxis2=dict(zeroline=False, showline=False, showticklabels=True, showgrid=True,\n",
    "                domain=[0.47, 1], side='top', dtick=25000),\n",
    "    legend=dict(x=0.029, y=1.038, font_size=10),\n",
    "    margin=dict(l=100, r=20, t=70, b=70),\n",
    "    paper_bgcolor='rgb(248, 248, 255)',\n",
    "    plot_bgcolor='rgb(248, 248, 255)',\n",
    ")\n",
    "\n",
    "# Add annotations to the plot\n",
    "annotations = []\n",
    "\n",
    "# Adding labels to charts\n",
    "for a_mf, a_pop, x_pcr in zip(np.round(sec1_2[\"df\"].head(10)[\"Males\"] / sec1_2[\"df\"].head(10)[\"Females\"], decimals=2),\n",
    "                              sec1_2[\"df\"].head(10)[\"Total\"],\n",
    "                              sec1_2[\"df\"].head(10)[\"postcode_region\"]):\n",
    "    # Add label to M/F ratio scatter plot\n",
    "    annotations.append(dict(xref='x2', yref='y2', y=x_pcr, x=a_mf,\n",
    "                            text=a_mf, xshift=50, showarrow=False))\n",
    "\n",
    "    # Add label to population bar chart\n",
    "    annotations.append(dict(xref='x1', yref='y1', y=x_pcr, x=a_pop,\n",
    "                            text=f\"{np.round(a_pop / 1000000, 3)}M\",\n",
    "                            xshift=25, showarrow=False))\n",
    "\n",
    "sec1_2[\"fig\"].update_layout(annotations=annotations)\n",
    "sec1_2[\"fig\"].show()\n",
    "\n",
    "# Remove variables to save memory\n",
    "del sec1_2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id='section1_3'></a>\n",
    "#### 1.3 Population by outward postcode using choropleth map\n",
    "Concat individual postcode geojson mapping into single variable `geojson_uk`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Declare section parent variable\n",
    "sec1_3 = dict()\n",
    "\n",
    "# Create parent geojson collection object\n",
    "sec1_3[\"geojson\"] = dict(type=\"FeatureCollection\", features=list())\n",
    "\n",
    "# Load all geojson files\n",
    "for f_geojson in list(dir_geojson.glob(\"*.geojson\")):\n",
    "    with open(f_geojson) as f:\n",
    "        geojson_data = json.load(f)\n",
    "        for feature in geojson_data[\"features\"]:\n",
    "            # Add feature id using properties.name, which is the outward postcode\n",
    "            feature[\"id\"] = feature[\"properties\"][\"name\"]\n",
    "\n",
    "            # Add feature to parent geojson collection\n",
    "            sec1_3[\"geojson\"][\"features\"].append(feature)\n",
    "\n",
    "# Sum up all numbers within same postcode outward area\n",
    "sec1_3[\"df\"] = var1[\"df\"].groupby([\"postcode_out\"]).sum().reset_index()\n",
    "\n",
    "# Get max population number among areas\n",
    "sec1_3[\"p_max\"] = sec1_3[\"df\"][\"Total\"].max()\n",
    "\n",
    "# Plot figure\n",
    "sec1_3[\"fig\"] = px.choropleth_mapbox(var1[\"df\"].groupby([\"postcode_out\"]).sum().reset_index(),\n",
    "                                     geojson=sec1_3[\"geojson\"],\n",
    "                                     locations='postcode_out', color='Total',\n",
    "                                     color_continuous_scale=\"jet\",\n",
    "                                     range_color=(0, sec1_3[\"p_max\"]),\n",
    "                                     mapbox_style=\"carto-positron\",\n",
    "                                     zoom=4.5, center={\"lat\": 52.5, \"lon\": -1.6},\n",
    "                                     opacity=0.5)\n",
    "sec1_3[\"fig\"].update_layout(margin=dict(r=0, t=0, l=0, b=0))\n",
    "sec1_3[\"fig\"].show()\n",
    "\n",
    "# Remove variables to save memory\n",
    "del sec1_3"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<small>As the opensource geographical data used here comes from Wikipedia, it does not cover all England regions. This leads to white areas on the map.</small>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id='section1_4'></a>\n",
    "#### 1.4 Population by postcode parent area using choropleth map\n",
    "In the above plot, regions are probably too granulated, so you would not be able to see an obvious trend. Let's merge them into parent regions."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Declare section parent variable\n",
    "sec1_4 = dict()\n",
    "\n",
    "# Create parent geojson collection object\n",
    "sec1_4[\"geojson\"] = dict(type=\"FeatureCollection\", features=list())\n",
    "\n",
    "# Load all geojson files\n",
    "for f_geojson in list(dir_geojson.glob(\"*.geojson\")):\n",
    "    with open(f_geojson) as f:\n",
    "        geojson_data = json.load(f)\n",
    "        # Merge granulated geometry areas into parent region\n",
    "        merged = unary_union(\n",
    "            list(map(\n",
    "                (lambda x: shapely.geometry.asShape(x[\"geometry\"])),\n",
    "                geojson_data[\"features\"])\n",
    "            ))\n",
    "        # Create new geojson feature from merged geometry\n",
    "        geojson_merged = geojson.Feature(geometry=merged,\n",
    "                                         properties={\"name\": f_geojson.stem},\n",
    "                                         id=f_geojson.stem)\n",
    "        # Add feature to parent geojson collection\n",
    "        sec1_4[\"geojson\"][\"features\"].append(geojson_merged)\n",
    "\n",
    "# Sum up all numbers within same postcode region\n",
    "sec1_4[\"df\"] = var1[\"df\"].groupby([\"postcode_region\"]).sum().reset_index()\n",
    "\n",
    "# Get max population number among regions\n",
    "sec1_4[\"p_max\"] = sec1_4[\"df\"][\"Total\"].max()\n",
    "\n",
    "# Plot figure\n",
    "sec1_4[\"fig\"] = px.choropleth_mapbox(sec1_4[\"df\"],\n",
    "                                     geojson=sec1_4[\"geojson\"],\n",
    "                                     locations='postcode_region',\n",
    "                                     color='Total',\n",
    "                                     color_continuous_scale=\"jet\",\n",
    "                                     range_color=(0, sec1_4[\"p_max\"]),\n",
    "                                     mapbox_style=\"carto-positron\",\n",
    "                                     zoom=4.5, center={\"lat\": 52.5, \"lon\": -1.6},\n",
    "                                     opacity=0.5)\n",
    "sec1_4[\"fig\"].update_layout(margin={\"r\": 0, \"t\": 0, \"l\": 0, \"b\": 0})\n",
    "sec1_4[\"fig\"].show()\n",
    "\n",
    "# Remove variables to save memory\n",
    "del sec1_4"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Remove section 1 variable\n",
    "del var1\n",
    "print(\"⚠ You have just deleted all variables for section 1.\"\n",
    "      \"If you would like to rerun any block in section 1, \"\n",
    "      \"please make sure that you run the section preparation block first.\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id='section2'></a>\n",
    "### 2. House prices and earnings"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Define a reusable function to read different sheets from the source data file."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define a function to read different sheets from source\n",
    "def read_df2(source, sheet, skip_top, column_name):\n",
    "    \"\"\"\n",
    "    A reusable function to read from the source file and process into a DataFrame in required format.\n",
    "    :param source: str or path object, where the source excel file located\n",
    "    :param sheet: str, name of the sheet in excel file to be read\n",
    "    :param skip_top: int, how many rows at the top of the sheet to be skipped\n",
    "    :param column_name: str, new column name to be given to the variable\n",
    "    :return: DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    # Define a function to transpose DataFrame row\n",
    "    def transpose_df2(r, c_name):\n",
    "        \"\"\"\n",
    "        To transpose a single row into DataFrame with specific format.\n",
    "        :param r: Series, a row from the DataFrame\n",
    "        :param c_name: str, new name to be given to transposed column\n",
    "        :return: DataFrame\n",
    "        \"\"\"\n",
    "        # row -> column -> new DataFrame\n",
    "        new_df = r[year_columns].T.to_frame().reset_index(drop=True)\n",
    "        # Rename the column\n",
    "        new_df.columns = [c_name]\n",
    "        # Add column [Region] and populate the whole column with value from cell [Region] in the row\n",
    "        new_df[\"Region\"] = r[\"Region\"].strip()\n",
    "        # Add column [Year] and populate the column from 2002 to 2020\n",
    "        new_df[\"Year\"] = range(2002, 2021)\n",
    "        return new_df\n",
    "\n",
    "    # Read target sheet from source file into DataFrame -> drop column [Code] -> drop row if any cell in column [Name] is empty\n",
    "    df = pd.read_excel(source, sheet_name=sheet, skiprows=skip_top).drop(columns=[\"Code\"]).dropna(subset=[\"Name\"])\n",
    "\n",
    "    # Drop empty columns\n",
    "    df.drop([col for col in df.columns if df[col].isnull().all()], axis=1, inplace=True)\n",
    "\n",
    "    # Standardise column names\n",
    "    year_columns = [str(x) for x in range(2002, 2021)]\n",
    "    df.columns = [\"Region\"] + year_columns\n",
    "\n",
    "    # Transpose and concat each row into the final DataFrame\n",
    "    df.loc[0][year_columns].T.reset_index(drop=True)\n",
    "    new_df = pd.DataFrame(columns=[\"Region\", \"Year\", column_name])\n",
    "    for _, row in df.iterrows():\n",
    "        new_df = pd.concat([new_df, transpose_df2(row, column_name)])\n",
    "\n",
    "    # Set numeric column data type to integer\n",
    "    return new_df.astype({column_name: 'int32'})\n",
    "\n",
    "# Declare section parent variable\n",
    "var2 = temp = dict()\n",
    "\n",
    "# Read sheet [1a] for the median house price by country and region, England and Wales, 2002 - 2020\n",
    "temp[\"df1a\"] = read_df2(file_house_price, \"1a\", 6, \"Median house price\")\n",
    "\n",
    "# Read sheet [1b] for the median gross annual earnings by country and region, England and Wales, 2002 - 2020\n",
    "temp[\"df1b\"] = read_df2(file_house_price, \"1b\", 6, \"Median earn\")\n",
    "\n",
    "# Read sheet [2a] for the lower quartile house price by country and region, England and Wales, 2002 - 2020\n",
    "temp[\"df2a\"] = read_df2(file_house_price, \"2a\", 6, \"Lower house price\")\n",
    "\n",
    "# Read sheet [1b] for the lower quartile gross annual earnings by country and region, England and Wales, 2002 - 2020\n",
    "temp[\"df2b\"] = read_df2(file_house_price, \"2b\", 6, \"Lower earn\")\n",
    "\n",
    "# Merge the four DataFrame above into new DataFrame\n",
    "var2[\"df\"] = pd.merge(temp[\"df1a\"], temp[\"df1b\"], on=[\"Region\", \"Year\"])\n",
    "var2[\"df\"] = pd.merge(var2[\"df\"], temp[\"df2a\"], on=[\"Region\", \"Year\"])\n",
    "var2[\"df\"] = pd.merge(var2[\"df\"], temp[\"df2b\"], on=[\"Region\", \"Year\"])\n",
    "# Show top 5 rows of processed DataFrame\n",
    "# df2_final.head(5)\n",
    "# Delete variables to save memory\n",
    "del temp\n",
    "\n",
    "print(\"✅ Section 2 preparation completed.\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id='section2_1'></a>\n",
    "#### 2.1 Correlation between median house price and median household earning\n",
    "\n",
    "- Scatter plot and trendlines show the correlations between median house price and median earn.\n",
    "- Top box plot shows the distribution of median house price.\n",
    "- Right rug box shows the distribution of median earn."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Declare section parent variable\n",
    "sec2_1 = dict()\n",
    "\n",
    "# Plot correlation\n",
    "sec2_1[\"fig\"] = px.scatter(var2[\"df\"][var2[\"df\"].Region.isin([\"England\", \"Wales\", \"London\"])],\n",
    "                           x=\"Median house price\", y=\"Median earn\", color=\"Region\",\n",
    "                           marginal_x=\"box\", marginal_y=\"rug\", trendline=\"ols\")\n",
    "sec2_1[\"fig\"].show()\n",
    "\n",
    "# Delete variables to save memory\n",
    "del sec2_1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id='section2_2'></a>\n",
    "#### 2.2 Median and lower quartile house prices from 2002 to 2020 (animated)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Declare section parent variable\n",
    "sec2_2 = dict()\n",
    "\n",
    "# Reformat dataframe into required format for plotting\n",
    "sec2_2[\"df\"] = pd.DataFrame(columns=[\"Region\", \"Year\", \"Variable\", \"Value\"])\n",
    "for _, row in var2[\"df\"].iterrows():\n",
    "    sec2_2[\"df\"] = sec2_2[\"df\"].append([dict(Region=row[\"Region\"], Year=row[\"Year\"],\n",
    "                                             Variable=\"Median house price\", Value=row[\"Median house price\"]),\n",
    "                                        dict(Region=row[\"Region\"], Year=row[\"Year\"],\n",
    "                                             Variable=\"Median earn\", Value=row[\"Median earn\"]),\n",
    "                                        dict(Region=row[\"Region\"], Year=row[\"Year\"],\n",
    "                                             Variable=\"Lower house price\", Value=row[\"Lower house price\"]),\n",
    "                                        dict(Region=row[\"Region\"], Year=row[\"Year\"],\n",
    "                                             Variable=\"Lower earn\", Value=row[\"Lower earn\"])],\n",
    "                                       ignore_index=True)\n",
    "\n",
    "# Generate grouped bar chart with animation based on year changes\n",
    "sec2_2[\"fig\"] = px.bar(sec2_2[\"df\"][sec2_2[\"df\"][\"Variable\"].isin([\"Median house price\", \"Lower house price\"])], x=\"Region\",\n",
    "                  y=\"Value\", text=\"Value\", color=\"Variable\", barmode=\"group\", animation_frame=\"Year\",\n",
    "                  animation_group=\"Region\", range_y=[0, 500000])\n",
    "sec2_2[\"fig\"].update_yaxes(title_text=\"Price\", secondary_y=False)\n",
    "sec2_2[\"fig\"].update_yaxes(title_text=\"Earn\", secondary_y=True)\n",
    "sec2_2[\"fig\"].show()\n",
    "\n",
    "# Delete variables to save memory\n",
    "del sec2_2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Remove section 2 variable\n",
    "del var2\n",
    "print(\"⚠ You have just deleted all variables for section 2.\"\n",
    "      \"If you would like to rerun any block in section 2, \"\n",
    "      \"please make sure that you run the section preparation block first.\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id='section3'></a>\n",
    "### 3. Text based analysis and visualisation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Declare section parent variable\n",
    "var3 = dict()\n",
    "\n",
    "# Load data and keep only selected columns\n",
    "var3[\"df\"] = pd.read_csv(file_tweet)[[\"user_name\", \"text\", \"date\"]].sort_values(by='date', ascending=False)\n",
    "\n",
    "# Convert date column to datetime type\n",
    "var3[\"df\"][\"date\"] = pd.to_datetime(var3[\"df\"][\"date\"], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "# Add new day column with only date from the date column, i.e. remove the time %H:%M:%S\n",
    "var3[\"df\"][\"day\"] = var3[\"df\"][\"date\"].apply(lambda x: x.date())\n",
    "\n",
    "# Use basic WhitespaceTokenizer, which will segment text into individual words simply using whitespace\n",
    "var3[\"tokenizer\"] = nltk.WhitespaceTokenizer()\n",
    "\n",
    "# Create a new column tokens\n",
    "var3[\"df\"][\"tokens\"] = var3[\"df\"][\"text\"].apply(lambda x: var3[\"tokenizer\"].tokenize(x.lower()))\n",
    "\n",
    "print(\"✅ Section 1 preparation completed.\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id='section3_1'></a>\n",
    "#### 3.1 Text tokenization and word cloud"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Declare section parent variable\n",
    "sec3_1 = dict()\n",
    "\n",
    "# Flatten the tokens into one list and remove some noise. In real case, this could combine with part-of-speech (POS) tagging. Here we only use a list of adjectives, conjunctions, articles and prepositions.\n",
    "sec3_1[\"noise\"] = [\"the\", \"a\", \"an\", \"from\", \"to\", \"at\", \"of\", \"on\", \"in\", \"and\", \"or\", \"is\", \"are\", \"was\", \"am\",\n",
    "                          \"were\", \"been\", \"have\", \"has\", \"had\", \"for\", \"with\", \"this\", \"that\", \"&amp;\", \"be\", \"as\", \"-\"]\n",
    "sec3_1[\"tokens\"] = [x for x in list(itertools.chain.from_iterable(var3[\"df\"][\"tokens\"].tolist())) if\n",
    "              not x.startswith(\"@\") and not x.startswith(\"http\") and x not in sec3_1[\"noise\"] and len(x) >= 3]\n",
    "\n",
    "# Calculate frequencies of each token\n",
    "sec3_1[\"freq\"] = nltk.FreqDist(sec3_1[\"tokens\"])\n",
    "\n",
    "# Create a word cloud\n",
    "sec3_1[\"cloud\"] = wordcloud.WordCloud(background_color='white', width=1200, height=600).generate_from_frequencies(\n",
    "    sec3_1[\"freq\"])\n",
    "sec3_1[\"fig\"] = px.imshow(sec3_1[\"cloud\"])\n",
    "sec3_1[\"fig\"].update_layout(xaxis={\"visible\": False}, yaxis={\"visible\": False})\n",
    "sec3_1[\"fig\"].show()\n",
    "\n",
    "# Delete variables to save memory\n",
    "del sec3_1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id='section3_2'></a>\n",
    "#### 3.2 Basic sentiment analysis with VADER\n",
    "Use nltk's built-in [VADER](https://ojs.aaai.org/index.php/ICWSM/article/view/14550) model to perform a basic sentiment analysis on these tweets without the need for training."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Declare section parent variable\n",
    "sec3_2 = dict()\n",
    "\n",
    "# Download lexicon resource required for using the VADER model\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# Create an analyzer instance\n",
    "sec3_2[\"analyzer\"] = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Create a function that returns a tuple of sentiment analysis result from the analyzer\n",
    "def sentiment_analysis(text):\n",
    "    result = sec3_2[\"analyzer\"].polarity_scores(text)\n",
    "    return result[\"neg\"], result[\"neu\"], result[\"pos\"], result[\"compound\"], \\\n",
    "           \"Positive\" if result[\"compound\"] > 0.05 else \"Negative\" if result[\"compound\"] < -0.05 else \"Neutral\"\n",
    "\n",
    "# Create new columns with sentiment analysis result\n",
    "var3[\"df\"][\"Negative\"], var3[\"df\"][\"Neutral\"], var3[\"df\"][\"Positive\"], var3[\"df\"][\"Compound\"], var3[\"df\"][\"Overall\"] = zip(\n",
    "    *var3[\"df\"][\"text\"].map(sentiment_analysis))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create pie chart of overall sentiment analysis result from the compound value\n",
    "sec3_2[\"fig1\"] = px.pie(var3[\"df\"].groupby(\"Overall\").count().reset_index(), values=\"user_name\", names=\"Overall\",\n",
    "                   title=\"Sentimental results\")\n",
    "sec3_2[\"fig1\"].show()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Unpivot the dataframe\n",
    "sec3_2[\"df1\"] = pd.melt(var3[\"df\"], id_vars=[\"user_name\", \"text\", \"tokens\", \"date\", \"Overall\", \"day\"],\n",
    "                     value_vars=[\"Negative\", \"Neutral\", \"Positive\", \"Compound\"])\n",
    "\n",
    "# Create box chart for distribution of negative scores across days\n",
    "sec3_2[\"fig2\"] = px.box(sec3_2[\"df1\"][(sec3_2[\"df1\"][\"Overall\"] == \"Negative\") & (sec3_2[\"df1\"][\"variable\"] == \"Negative\")], x=\"day\",\n",
    "             y=\"value\", labels={\n",
    "        \"value\": \"Negative score\"\n",
    "    })\n",
    "sec3_2[\"fig2\"].show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sec3_2[\"df2\"] = var3[\"df\"].groupby([\"Overall\", \"day\"]).count().reset_index().sort_values(by=\"day\")\n",
    "\n",
    "sec3_2[\"fig3\"] = px.line(sec3_2[\"df2\"], x=\"day\", y=\"user_name\", color=\"Overall\",\n",
    "              title=\"Positive/Negative/Neutral tweet counts over days\", labels={\n",
    "        \"user_name\": \"Count\"\n",
    "    })\n",
    "sec3_2[\"fig3\"].update_layout()\n",
    "sec3_2[\"fig3\"].show()\n",
    "\n",
    "# Delete variables to save memory\n",
    "del sec3_2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Remove section 2 variable\n",
    "del var3\n",
    "print(\"⚠ You have just deleted all variables for section 3.\"\n",
    "      \"If you would like to rerun any block in section 3, \"\n",
    "      \"please make sure that you run the section preparation block first.\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}